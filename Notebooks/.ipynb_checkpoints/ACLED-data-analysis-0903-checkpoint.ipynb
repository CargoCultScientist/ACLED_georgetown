{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "sns.set_theme(style=\"darkgrid\",palette=sns.color_palette(\"Set2\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['data_id'] not in index\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_11588/4102569648.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex_col\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'data_id'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'country'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'event_type'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'sub_event_type'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'fatalities'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'notes'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'region'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'latitude'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'longitude'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3459\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mis_iterator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3460\u001b[0m                 \u001b[0mkey\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3461\u001b[1;33m             \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_listlike_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3462\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3463\u001b[0m         \u001b[1;31m# take() does not accept boolean indexers\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_get_listlike_indexer\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m   1312\u001b[0m             \u001b[0mkeyarr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnew_indexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0max\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reindex_non_unique\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkeyarr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1313\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1314\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_read_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkeyarr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1315\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1316\u001b[0m         if needs_i8_conversion(ax.dtype) or isinstance(\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_validate_read_indexer\u001b[1;34m(self, key, indexer, axis)\u001b[0m\n\u001b[0;32m   1375\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1376\u001b[0m             \u001b[0mnot_found\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mensure_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmissing_mask\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnonzero\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munique\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1377\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"{not_found} not in index\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1378\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1379\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: \"['data_id'] not in index\""
     ]
    }
   ],
   "source": [
    "url = 'https://raw.githubusercontent.com/georgetown-analytics/ACLED/main/CSV_Main/2020-06-01-2021-06-01-Eastern_Africa-Middle_Africa-Northern_Africa-Southern_Africa-Western_Africa.csv'\n",
    "data = pd.read_csv(url, index_col=0)\n",
    "data.reset\n",
    "data=data[['data_id','country','event_type','sub_event_type','fatalities','notes','region','latitude','longitude']]\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# basic data explore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of records\n",
    "\n",
    "print(\"event region count:\",data.shape[0])\n",
    "print(\"event region count:\",data[\"region\"].nunique())\n",
    "print(\"event country count:\",data[\"country\"].nunique())\n",
    "print(\"event type count:\",data[\"event_type\"].nunique())\n",
    "print(\"event sub_event_type count:\",data[\"sub_event_type\"].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"event_type\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# event distribution by event_type and region\n",
    "\n",
    "region_event_count=data.pivot_table(index=\"region\",\n",
    "                 columns=[\"event_type\"],\n",
    "                 values=\"data_id\",\n",
    "                 aggfunc=\"count\",\n",
    "                 margins=True)\n",
    "\n",
    "region_event_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comment#1ï¼š \n",
    "\n",
    "This table counts the number of event type that occur by region. Which region and which event type occur more"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,6))\n",
    "\n",
    "region_event_count2=data.pivot_table(index=\"region\",\n",
    "                 columns=[\"event_type\"],\n",
    "                 values=\"data_id\",\n",
    "                 aggfunc=\"count\",\n",
    "                 margins=False)\n",
    "\n",
    "_=sns.heatmap(region_event_count2, \n",
    "            cmap=\"YlGnBu\",\n",
    "            annot=True,\n",
    "              fmt='d')\n",
    "_=plt.xticks(rotation=-90)\n",
    "_=plt.title(\"event distribution by event_type and region\",fontsize=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comment#2:\n",
    "    \n",
    "To visualize the values from the previous statistics table, the darker the color, the more times the event occurred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fatalities distribution by event_type and region\n",
    "\n",
    "region_fatality_count=data.pivot_table(index=\"region\",\n",
    "                 columns=[\"event_type\"],\n",
    "                 values=\"fatalities\",\n",
    "                 aggfunc=\"sum\",\n",
    "                 margins=True)\n",
    "region_fatality_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comment#3:\n",
    "\n",
    "Event type: Number of casualties caused by event type fatalities, also used to measure the impact of what event in what place"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,6))\n",
    "\n",
    "region_fatality_count2=data.pivot_table(index=\"region\",\n",
    "                 columns=[\"event_type\"],\n",
    "                 values=\"fatalities\",\n",
    "                 aggfunc=\"sum\",\n",
    "                 margins=False)\n",
    "\n",
    "_=sns.heatmap(region_fatality_count2, \n",
    "            cmap=\"YlGnBu\",\n",
    "            annot=True,\n",
    "              fmt='d')\n",
    "_=plt.xticks(rotation=-90)\n",
    "_=plt.title(\"fatalities distribution by event_type and region\",fontsize=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comment#4:\n",
    "\n",
    "Again, visualize the data in the above table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# event type distribution by geo\n",
    "\n",
    "plt.figure(figsize=(9,6))\n",
    "_=sns.scatterplot(x=\"latitude\",\n",
    "                  y=\"longitude\",\n",
    "                  hue=\"event_type\",\n",
    "                  data=data)\n",
    "_=plt.title(\"events geo distribution by event type\",fontsize=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comment#5:\n",
    "    \n",
    "Events are shown by latitude and longitude, with different colors representing different types of events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if we only consider Protests and Battles type\n",
    "\n",
    "plt.figure(figsize=(9,6))\n",
    "_=sns.scatterplot(x=\"latitude\",\n",
    "                  y=\"longitude\",\n",
    "                  hue=\"event_type\",\n",
    "                  data=data[data[\"event_type\"].isin([\"Protests\",\"Battles\"])])\n",
    "_=plt.title(\"events geo distribution by event type\",fontsize=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comment#6:\n",
    "    \n",
    "The same latitude and longitude is shown, but only the \"Protests\" and \"Battles\" event types are included here, because I see that your crew has only cleaned the data for these two events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# clean notes\n",
    "\n",
    "+ Take out the data that will be used for modeling, do cleaning feature engineering, etc\n",
    "+ x=notes y=event_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string  \n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize,pos_tag\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sub=data[[\"event_type\",\"notes\"]].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove date info in notes\n",
    "\n",
    "data_sub[\"notes\"].map(lambda x: x.split(\",\")[0]).value_counts().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What it looks like when it's gone\n",
    "\n",
    "data_sub[\"notes\"].map(lambda x: \",\".join(x.split(\",\")[1:])).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sub[\"notes\"]=data_sub[\"notes\"].map(lambda x: \",\".join(x.split(\",\")[1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sub.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comment#7:\n",
    "    \n",
    "Here is the description of the date before the notes, because I look at the first sentence of every Notes is the time, so the first comma before the first comma is removed according to the position of the first comma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define cleaning function \n",
    "\n",
    "# Remove extra space, word segmentation, part-of-speech tagging\n",
    "def tokenize(sentence):\n",
    "    sentence = re.sub(r'\\s+', ' ', sentence)\n",
    "    token_words = word_tokenize(sentence)\n",
    "    token_words = pos_tag(token_words)   \n",
    "    return token_words\n",
    "\n",
    "# Normalize the word form\n",
    "def stem(token_words):\n",
    "    wordnet_lematizer = WordNetLemmatizer()\n",
    "    words_lematizer = []\n",
    "    for word, tag in token_words:\n",
    "        if tag.startswith('NN'):\n",
    "            word_lematizer =  wordnet_lematizer.lemmatize(word, pos='n')  # n-noun\n",
    "        elif tag.startswith('VB'): \n",
    "            word_lematizer =  wordnet_lematizer.lemmatize(word, pos='v')   # v-verb\n",
    "        elif tag.startswith('JJ'): \n",
    "            word_lematizer =  wordnet_lematizer.lemmatize(word, pos='a')   # adjective\n",
    "        elif tag.startswith('R'): \n",
    "            word_lematizer =  wordnet_lematizer.lemmatize(word, pos='r')   # r-pronoun\n",
    "        else: \n",
    "            word_lematizer =  wordnet_lematizer.lemmatize(word)\n",
    "        words_lematizer.append(word_lematizer)\n",
    "    return words_lematizer\n",
    "\n",
    "# Remove stopwords \n",
    "def delete_stopwords(token_words):\n",
    "    cleaned_words = [word for word in token_words if word not in stopwords.words('english')]\n",
    "    return cleaned_words\n",
    "\n",
    "# Determine whether the string is a number\n",
    "def is_number(s):\n",
    "    try:\n",
    "        float(s)\n",
    "        return True\n",
    "    except ValueError:\n",
    "        pass\n",
    " \n",
    "    try:\n",
    "        import unicodedata\n",
    "        unicodedata.numeric(s)\n",
    "        return True\n",
    "    except (TypeError, ValueError):\n",
    "        pass\n",
    " \n",
    "    return False\n",
    "\n",
    "# Remove Special words and numbers\n",
    "def delete_characters(token_words):\n",
    "    words_list = [word for word in token_words if word not in string.punctuation and not is_number(word)]\n",
    "    return words_list\n",
    "\n",
    "# Change to lowercase\n",
    "def to_lower(token_words):\n",
    "    words_lists = [x.lower() for x in token_words]\n",
    "    return words_lists\n",
    "\n",
    "# Combine the above steps and perform text preprocessing\n",
    "def pre_process(text):\n",
    "    token_words = tokenize(text)\n",
    "    token_words = to_lower(token_words)\n",
    "    token_words = stem(token_words)    \n",
    "    token_words = delete_stopwords(token_words)\n",
    "    token_words = delete_characters(token_words)    \n",
    "    cleaned_text=\" \".join(token_words)\n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comment#8:\n",
    "    \n",
    "To clean the text in Notes using the NLTK package, do the following:\n",
    "1. Tokenize: Use the word_tokenize participle first, followed by pos_tag for each word\n",
    "2. Lemmatize: Use WordNetLemmatizer to restore lemmatize to the result of the previous participle\n",
    "3. Remove stopwords: the definition of stopwords is the English stopwords defined in NLTK. Corpus\n",
    "4. Remove punctuation or numbers. Punctuation is defined as the punctuation contained in string. string\n",
    "5. Keep all the words in lower case\n",
    "6. Finally merge the cleaned words into a piece of text with Spaces separating each word.\n",
    "The above code defines each cleaning step as a function that is called to perform the cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# apply clean\n",
    "\n",
    "data_sub[\"notes_clean\"]=data_sub[\"notes\"].map(lambda x: pre_process(x))\n",
    "data_sub.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ These two actions take a long time to execute because there are so many steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# show keywords distribution by event_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate notes text by event_type\n",
    "\n",
    "event_type_notes=data_sub.groupby(\"event_type\")[\"notes_clean\"].apply(list).to_dict()\n",
    "print(event_type_notes.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comment#9:\n",
    "    \n",
    "The following is the notes word cloud map for each event according to event type. The main purpose is to see the differences in the use of words in notes description under different event types, and what kind of words are used more in what events. If the classification model of Event type is to be made based on notes content, it is feasible to show the difference of notes description words in different event types first\n",
    "From the word cloud map, we can see that there are differences in the use of different event types "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "for k in event_type_notes.keys():\n",
    "    \n",
    "    text=\" \".join(event_type_notes[k])\n",
    "\n",
    "    wc = WordCloud(width=900, height=500, \n",
    "                   mode='RGBA', background_color=None,\n",
    "                   min_font_size=12,max_font_size=72,\n",
    "                   collocations=False,prefer_horizontal=0.8,\n",
    "                   max_words=100).generate(text)\n",
    "\n",
    "    # show wordcloud img\n",
    "    plt.figure(figsize=(9,5),dpi=100)\n",
    "    plt.imshow(wc, interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    plt.title(\"word cloud of event type= {} \\n{}\".format(k,'- '*40),fontsize=24,color='red')\n",
    "    plt.show()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# classification model \n",
    "\n",
    "+ [\"Protests\",\"Battles\"]\n",
    "+ Classification model of these two kinds of events"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## classification model(only 2 event types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "\n",
    "# from sklearn.naive_bayes import MultinomialNB\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report,confusion_matrix,roc_curve,auc,accuracy_score,f1_score\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define two functions that will be used later in the drawing\n",
    "\n",
    "def plotConfusionMaxtrix(confmat_data=None,xlabel='',ylabel='',title='',cmap=plt.cm.Blues,plt_ax=None):\n",
    "    \"\"\"\n",
    "    Used for drawing - confusion matrix\n",
    "    \"\"\"\n",
    "    plt_ax.matshow(confmat_data, cmap=cmap, alpha=0.75)\n",
    "    for i in range(confmat_data.shape[0]):\n",
    "        for j in range(confmat_data.shape[1]):\n",
    "            plt_ax.text(x=j, y=i,s=confmat_data[i, j],va='center', ha='center',fontsize=12)\n",
    "    plt_ax.set_xlabel(xlabel,fontsize=12)\n",
    "    plt_ax.set_ylabel(ylabel,fontsize=12)\n",
    "    plt_ax.set_title(title,fontsize=12)\n",
    "    return \n",
    "\n",
    "\n",
    "def plotRocBinary(fpr=None,tpr=None,roc_auc=None,color='red',title='',plot_ax=None):\n",
    "    plot_ax.plot(fpr, tpr, c=color,linewidth=2,label='ROC Curve (auc = %0.3f)' % roc_auc)\n",
    "    plot_ax.plot([0, 1], [0, 1], 'k--',label='Random Prediction')\n",
    "    plot_ax.set_xlim([0.0, 1.0])\n",
    "    plot_ax.set_ylim([0.0, 1.0])\n",
    "    plot_ax.set_xlabel('1 - Specifity',fontsize=12)\n",
    "    plot_ax.set_ylabel('Sensitivity',fontsize=12)\n",
    "    plot_ax.set_title(title,fontsize=12)\n",
    "    for key in [\"left\",\"right\",\"top\",\"bottom\"]:\n",
    "        plot_ax.spines[key].set_alpha(0.3)\n",
    "    plot_ax.legend(loc=\"lower right\")\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perpare x and y \n",
    "data_model=data_sub[data_sub[\"event_type\"].isin([\"Protests\",\"Battles\"])][[\"notes_clean\",\"event_type\"]].copy()\n",
    "x=data_model[\"notes_clean\"]\n",
    "y=data_model[\"event_type\"].map({\"Protests\":0,\"Battles\":1})\n",
    "\n",
    "# split train set and test set\n",
    "xtrain,xtest,ytrain,ytest=train_test_split(x,y,test_size=0.3,random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract text features using TFIDF\n",
    "\n",
    "vectorizer = TfidfVectorizer(min_df=3,max_df=0.3)\n",
    "xtrain_vectorizer=vectorizer.fit_transform(xtrain)\n",
    "xtest_vectorizer =vectorizer.transform(xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random forest was used as the classification model, and the super parameters were adjusted to make the model best\n",
    "\n",
    "rf=RandomForestClassifier(random_state=42)\n",
    "\n",
    "# set tune parameter\n",
    "parameters = {'max_depth': range(3,51)}\n",
    "\n",
    "grid_rf = GridSearchCV(rf, parameters,cv=5, n_jobs=-1)\n",
    "grid_rf.fit(xtrain_vectorizer,ytrain)\n",
    "\n",
    "# The optimal depth is 38\n",
    "print(\"best_params: \",grid_rf.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ This section also takes a long time to do hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the model's classification report on training data and test data\n",
    "\n",
    "pred_ytrain_grid_rf=grid_rf.predict(xtrain_vectorizer)\n",
    "pred_ytest_grid_rf=grid_rf.predict(xtest_vectorizer)\n",
    "\n",
    "proba_ytrain_grid_rf=grid_rf.predict_proba(xtrain_vectorizer)\n",
    "proba_ytest_grid_rf=grid_rf.predict_proba(xtest_vectorizer)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\" \"*5,\"Train Data Classification Report\")\n",
    "print(classification_report(ytrain,pred_ytrain_grid_rf))\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\" \"*5,\"Test Data Classification Report\")\n",
    "print(classification_report(ytest,pred_ytest_grid_rf))\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ This seems to work very well for both Battles and Protests, and the accuracy of both is 100% in the test set\n",
    "+ Notice the previous definition: 0 means Protests, 1 means Battles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the confusion matrix\n",
    "\n",
    "confmat_train = confusion_matrix(y_true=ytrain, y_pred=pred_ytrain_grid_rf)\n",
    "confmat_test = confusion_matrix(y_true=ytest, y_pred=pred_ytest_grid_rf)\n",
    "\n",
    "# Plot Confusion Matrix\n",
    "fig, (ax1, ax2) = plt.subplots(nrows=1,ncols=2,figsize=(16, 7))\n",
    "#plot for train data\n",
    "plotConfusionMaxtrix(confmat_data=confmat_train,\n",
    "                     xlabel='Predicted Label',\n",
    "                     ylabel='True Label',\n",
    "                     title='Train Data Confusion Matrix',\n",
    "                     cmap=plt.cm.Blues,\n",
    "                     plt_ax=ax1)\n",
    "#plot for test data\n",
    "plotConfusionMaxtrix(confmat_data=confmat_test,\n",
    "                     xlabel='Predicted Label',\n",
    "                     ylabel='True Label',\n",
    "                     title='Test Data Confusion Matrix',\n",
    "                     cmap=plt.cm.Greens,\n",
    "                     plt_ax=ax2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ A few predictions were wrong, but the overall model worked pretty well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# roc curve\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(nrows=1,ncols=2,figsize=(16, 7))\n",
    "\n",
    "# Plot ROC curve - train data\n",
    "fpr, tpr, thresholds = roc_curve(ytrain, proba_ytrain_grid_rf[:,1])\n",
    "roc_auc = auc(fpr,tpr)\n",
    "plotRocBinary(fpr=fpr,tpr=tpr,roc_auc=roc_auc,color='red',title='ROC: Train Data',plot_ax=ax1)\n",
    "\n",
    "# Plot ROC curve - test data\n",
    "fpr, tpr, thresholds = roc_curve(ytest, proba_ytest_grid_rf[:,1])\n",
    "roc_auc = auc(fpr,tpr)\n",
    "plotRocBinary(fpr=fpr,tpr=tpr,roc_auc=roc_auc,color='red',title='ROC: Test Data',plot_ax=ax2)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ roc curve is also very good"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Overall, the effect of the dichotomy model of the two events is very good. Based on the contents of Notes, it can perfectly identify whether an event is protest or battle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## classification model(6 event types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function that defines the ROC of a multi-classification problem The ROC of multi-classification is not the same as the ROC of dichotomies\n",
    "\n",
    "def plotRocMulti(fpr=None,tpr=None,roc_auc=None,title='',plot_ax=None):\n",
    "    # plot Roc for each category\n",
    "    colors=['peru','darkcyan','cyan','purple','red','blue']\n",
    "    for i, color in zip(range(6), colors):\n",
    "        plot_ax.plot(fpr[i], tpr[i], color=color, lw=2,label='ROC curve of Class:{0}(auc = {1:0.3f})'.format(i, roc_auc[i]))\n",
    "\n",
    "    plot_ax.plot([0, 1], [0, 1], 'k--', lw=1)\n",
    "    plot_ax.set_xlim([0.0, 1.0])\n",
    "    plot_ax.set_ylim([0.0, 1.0])\n",
    "    plot_ax.set_xlabel('1 - Specifity',fontsize=12)\n",
    "    plot_ax.set_ylabel('Sensitivity',fontsize=12)\n",
    "    plot_ax.set_title(title,fontsize=12)\n",
    "    for key in [\"left\",\"right\",\"top\",\"bottom\"]:\n",
    "        plot_ax.spines[key].set_alpha(0.3)\n",
    "    plot_ax.legend(loc=\"lower right\")\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perpare x and y \n",
    "data_model_6=data_sub[[\"notes_clean\",\"event_type\"]].copy()\n",
    "x=data_model_6[\"notes_clean\"]\n",
    "y=data_model_6[\"event_type\"].map({\"Protests\":0,\n",
    "                                  \"Battles\":1,\n",
    "                                  \"Violence against civilians\":2,\n",
    "                                  \"Riots\":3,\n",
    "                                  \"Strategic developments\":4,\n",
    "                                  \"Explosions/Remote violence\":5})\n",
    "\n",
    "# split train set and test set\n",
    "xtrain,xtest,ytrain,ytest=train_test_split(x,y,test_size=0.3,random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many records are there for each of the six event types? Now it seems that the sample size of each category is quite different. Let's try modeling first (because the previous two classification models are very effective)\n",
    "\n",
    "data_model_6[\"event_type\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract text features using TFIDF\n",
    "\n",
    "vectorizer6 = TfidfVectorizer(min_df=3,max_df=0.3)\n",
    "xtrain_vectorizer6=vectorizer6.fit_transform(xtrain)\n",
    "xtest_vectorizer6 =vectorizer6.transform(xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random forest was used as the classification model, and the super parameters were adjusted to make the model best\n",
    "\n",
    "rf=RandomForestClassifier(random_state=42)\n",
    "\n",
    "# set tune parameter\n",
    "parameters = {'max_depth': range(5,101,2)}\n",
    "\n",
    "grid_rf6 = GridSearchCV(rf, parameters,cv=5, n_jobs=-1)\n",
    "grid_rf6.fit(xtrain_vectorizer6,ytrain)\n",
    "\n",
    "# The optimal depth is 97\n",
    "print(\"best_params: \",grid_rf6.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the model's classification report on training data and test data\n",
    "\n",
    "pred_ytrain_grid_rf6=grid_rf6.predict(xtrain_vectorizer6)\n",
    "pred_ytest_grid_rf6=grid_rf6.predict(xtest_vectorizer6)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\" \"*5,\"Train Data Classification Report\")\n",
    "print(classification_report(ytrain,pred_ytrain_grid_rf6))\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\" \"*5,\"Test Data Classification Report\")\n",
    "print(classification_report(ytest,pred_ytest_grid_rf6))\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ The overall accuracy of the model is still 91% on the test set (but the sample is not balanced. The best we can do is refer to f1-Score.)\n",
    "+ Explosions and Explosions Explosions and Remote violence are the main types of Explosions and Explosions in this week's Explosions and Explosions Explosions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the confusion matrix\n",
    "\n",
    "confmat_train6 = confusion_matrix(y_true=ytrain, y_pred=pred_ytrain_grid_rf6)\n",
    "confmat_test6 = confusion_matrix(y_true=ytest, y_pred=pred_ytest_grid_rf6)\n",
    "\n",
    "# Plot Confusion Matrix\n",
    "fig, (ax1, ax2) = plt.subplots(nrows=1,ncols=2,figsize=(16, 7))\n",
    "#plot for train data\n",
    "plotConfusionMaxtrix(confmat_data=confmat_train6,\n",
    "                     xlabel='Predicted Label',\n",
    "                     ylabel='True Label',\n",
    "                     title='Train Data Confusion Matrix',\n",
    "                     cmap=plt.cm.Blues,\n",
    "                     plt_ax=ax1)\n",
    "#plot for test data\n",
    "plotConfusionMaxtrix(confmat_data=confmat_test6,\n",
    "                     xlabel='Predicted Label',\n",
    "                     ylabel='True Label',\n",
    "                     title='Test Data Confusion Matrix',\n",
    "                     cmap=plt.cm.Greens,\n",
    "                     plt_ax=ax2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import label_binarize\n",
    "\n",
    "proba_ytrain_grid_rf6=grid_rf6.predict_proba(xtrain_vectorizer6)\n",
    "proba_ytest_grid_rf6=grid_rf6.predict_proba(xtest_vectorizer6)\n",
    "\n",
    "# Training data ROC\n",
    "train_fpr = dict()\n",
    "train_tpr = dict()\n",
    "train_roc_auc = dict()\n",
    "ytrain_bin = label_binarize(ytrain, classes=[0, 1, 2,3,4,5])\n",
    "# Calculate the FPR TPR AUC for each category\n",
    "for i in range(6):\n",
    "    train_fpr[i], train_tpr[i], _ = roc_curve(ytrain_bin[:, i], proba_ytrain_grid_rf6[:, i])\n",
    "    train_roc_auc[i] = auc(train_fpr[i], train_tpr[i])\n",
    "    \n",
    "# Test the ROC of data\n",
    "test_fpr = dict()\n",
    "test_tpr = dict()\n",
    "test_roc_auc = dict()\n",
    "ytest_bin = label_binarize(ytest, classes=[0, 1, 2,3,4,5])\n",
    "# Calculate the FPR TPR AUC for each category\n",
    "for i in range(6):\n",
    "    test_fpr[i], test_tpr[i], _ = roc_curve(ytest_bin[:, i], proba_ytest_grid_rf6[:, i])\n",
    "    test_roc_auc[i] = auc(test_fpr[i], test_tpr[i])\n",
    "    \n",
    "# draw\n",
    "fig, (ax1, ax2) = plt.subplots(nrows=1,ncols=2,figsize=(16, 7))\n",
    "plotRocMulti(fpr=train_fpr,tpr=train_tpr,roc_auc=train_roc_auc,title='ROC: Train Data',plot_ax=ax1)\n",
    "plotRocMulti(fpr=test_fpr,tpr=test_tpr,roc_auc=test_roc_auc,title='ROC: Test Data',plot_ax=ax2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## classification model(6 event types deal with imbalanced problem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imblearn Used to deal with unbalanced sample problems\n",
    "\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A balanced sample training model is constructed by random falling sampling\n",
    "\n",
    "# The first step in building a pipeline is to do random drop sampling and the second step is classifier\n",
    "rf_pipe = Pipeline(\n",
    "    [\n",
    "        ('sampling', RandomUnderSampler(random_state=42)),\n",
    "        ('classifier', RandomForestClassifier(random_state=42))   \n",
    "    ])\n",
    "\n",
    "# search for best parameters by GridSearchCV\n",
    "parameters = {'classifier__max_depth': range(5,101,2)}\n",
    "\n",
    "grid_rf_pipe = GridSearchCV(rf_pipe, parameters,cv=5,n_jobs=-1)\n",
    "grid_rf_pipe.fit(xtrain_vectorizer6,ytrain)\n",
    "\n",
    "# The optimal depth is 95\n",
    "print(\"best_params: \",grid_rf_pipe.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the model's classification report on training data and test data\n",
    "\n",
    "pred_ytrain_grid_rf_pipe=grid_rf_pipe.predict(xtrain_vectorizer6)\n",
    "pred_ytest_grid_rf_pipe=grid_rf_pipe.predict(xtest_vectorizer6)\n",
    "\n",
    "proba_ytrain_grid_rf_pipe=grid_rf_pipe.predict_proba(xtrain_vectorizer6)\n",
    "proba_ytest_grid_rf_pipe=grid_rf_pipe.predict_proba(xtest_vectorizer6)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\" \"*5,\"Train Data Classification Report\")\n",
    "print(classification_report(ytrain,pred_ytrain_grid_rf_pipe))\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\" \"*5,\"Test Data Classification Report\")\n",
    "print(classification_report(ytest,pred_ytest_grid_rf_pipe))\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the confusion matrix\n",
    "\n",
    "confmat_train_pipe = confusion_matrix(y_true=ytrain, y_pred=pred_ytrain_grid_rf_pipe)\n",
    "confmat_test_pipe = confusion_matrix(y_true=ytest, y_pred=pred_ytest_grid_rf_pipe)\n",
    "\n",
    "# Plot Confusion Matrix\n",
    "fig, (ax1, ax2) = plt.subplots(nrows=1,ncols=2,figsize=(16, 7))\n",
    "#plot for train data\n",
    "plotConfusionMaxtrix(confmat_data=confmat_train_pipe,\n",
    "                     xlabel='Predicted Label',\n",
    "                     ylabel='True Label',\n",
    "                     title='Train Data Confusion Matrix',\n",
    "                     cmap=plt.cm.Blues,\n",
    "                     plt_ax=ax1)\n",
    "#plot for test data\n",
    "plotConfusionMaxtrix(confmat_data=confmat_test_pipe,\n",
    "                     xlabel='Predicted Label',\n",
    "                     ylabel='True Label',\n",
    "                     title='Test Data Confusion Matrix',\n",
    "                     cmap=plt.cm.Greens,\n",
    "                     plt_ax=ax2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ It is easy to see from the results of the confusion matrix of the test data that the values of the main diagonals of some events increase and some decrease\n",
    "+ If we expect events of the latter two categories (with smaller sample size) to be accurately identified, then we use the model processed with unbalanced samples. If there is no preference for the recognition of each category, we use the model processed with balanced samples, because the samples of the latter two event categories are relatively small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the probability of each class of events\n",
    "proba_ytrain_grid_rf_pipe=grid_rf_pipe.predict_proba(xtrain_vectorizer6)\n",
    "proba_ytest_grid_rf_pipe=grid_rf_pipe.predict_proba(xtest_vectorizer6)\n",
    "\n",
    "# Training data ROC\n",
    "train_fpr = dict()\n",
    "train_tpr = dict()\n",
    "train_roc_auc = dict()\n",
    "ytrain_bin = label_binarize(ytrain, classes=[0, 1, 2,3,4,5])\n",
    "# Calculate the FPR TPR AUC for each category\n",
    "for i in range(6):\n",
    "    train_fpr[i], train_tpr[i], _ = roc_curve(ytrain_bin[:, i], proba_ytrain_grid_rf_pipe[:, i])\n",
    "    train_roc_auc[i] = auc(train_fpr[i], train_tpr[i])\n",
    "    \n",
    "# Test the ROC of data\n",
    "test_fpr = dict()\n",
    "test_tpr = dict()\n",
    "test_roc_auc = dict()\n",
    "ytest_bin = label_binarize(ytest, classes=[0, 1, 2,3,4,5])\n",
    "# Calculate the FPR TPR AUC for each category\n",
    "for i in range(6):\n",
    "    test_fpr[i], test_tpr[i], _ = roc_curve(ytest_bin[:, i], proba_ytest_grid_rf_pipe[:, i])\n",
    "    test_roc_auc[i] = auc(test_fpr[i], test_tpr[i])\n",
    "    \n",
    "# draw\n",
    "fig, (ax1, ax2) = plt.subplots(nrows=1,ncols=2,figsize=(16, 7))\n",
    "plotRocMulti(fpr=train_fpr,tpr=train_tpr,roc_auc=train_roc_auc,title='ROC: Train Data',plot_ax=ax1)\n",
    "plotRocMulti(fpr=test_fpr,tpr=test_tpr,roc_auc=test_roc_auc,title='ROC: Test Data',plot_ax=ax2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Overall, the model still performs well. If more training data can be added, several events with relatively small sample size will be more accurate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
