{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f2d84d3c-9db1-4c18-ad17-b0dbeb8e017f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf-idf_score_matrix\n",
      "           pro_doc   bat_doc   erv_doc  riot_doc  strdev_doc   vac_doc\n",
      "aba       0.000000  0.000963  0.000000  0.000000    0.000000  0.003324\n",
      "ababa     0.026436  0.000652  0.055470  0.022091    0.067777  0.014397\n",
      "abadu     0.000000  0.000000  0.000000  0.005530    0.000000  0.000000\n",
      "abala     0.006976  0.000000  0.000000  0.004534    0.000000  0.000000\n",
      "abatimbo  0.000000  0.000587  0.000000  0.000000    0.000000  0.000000\n",
      "...            ...       ...       ...       ...         ...       ...\n",
      "zigem     0.000000  0.000587  0.000000  0.000000    0.000000  0.000000\n",
      "ziway     0.003927  0.000000  0.000000  0.003828    0.000000  0.002807\n",
      "zonal     0.000000  0.000000  0.000000  0.000000    0.030537  0.000000\n",
      "zone      0.042801  0.016030  0.069337  0.027000    0.162665  0.149371\n",
      "zufan     0.000000  0.000294  0.000000  0.000000    0.000000  0.000000\n",
      "\n",
      "[2693 rows x 6 columns]\n",
      "term_freq(tf)_matrix\n",
      "          pro_doc  bat_doc  erv_doc  riot_doc  strdev_doc  vac_doc\n",
      "aba             0        4        0         0           0        1\n",
      "ababa          21        5        4         9           5        8\n",
      "abadu           0        0        0         1           0        0\n",
      "abala           3        0        0         1           0        0\n",
      "abatimbo        0        2        0         0           0        0\n",
      "...           ...      ...      ...       ...         ...      ...\n",
      "zigem           0        2        0         0           0        0\n",
      "ziway           2        0        0         1           0        1\n",
      "zonal           0        0        0         0           1        0\n",
      "zone           34      123        5        11          12       83\n",
      "zufan           0        1        0         0           0        0\n",
      "\n",
      "[2693 rows x 6 columns]\n",
      "Cluster 0:\n",
      " ongoing\n",
      " repeated\n",
      " nov\n",
      " conflict\n",
      " tigray\n",
      " event\n",
      " violence\n",
      " tdf\n",
      " acled\n",
      " affected\n",
      "Cluster 1:\n",
      " size\n",
      " report\n",
      " rioter\n",
      " demonstration\n",
      " demonstrator\n",
      " killed\n",
      " oromo\n",
      " popular\n",
      " police\n",
      " tire\n",
      "Cluster 2:\n",
      " killed\n",
      " shot\n",
      " civilian\n",
      " oromia\n",
      " unidentified\n",
      " ethnic\n",
      " force\n",
      " tigray\n",
      " zone\n",
      " amhara\n",
      "Cluster 3:\n",
      " arrest\n",
      " seizure\n",
      " around\n",
      " police\n",
      " seized\n",
      " weapon\n",
      " oromia\n",
      " zone\n",
      " arrested\n",
      " supporting\n",
      "Cluster 4:\n",
      " grenade\n",
      " force\n",
      " tigray\n",
      " tplf\n",
      " ethiopian\n",
      " killed\n",
      " conducted\n",
      " fatality\n",
      " wa\n",
      " thrown\n",
      "Cluster 5:\n",
      " size\n",
      " report\n",
      " protester\n",
      " demonstration\n",
      " held\n",
      " protest\n",
      " denouncing\n",
      " amhara\n",
      " demonstrator\n",
      " student\n",
      "\n",
      "\n",
      "prediction data:\n",
      "2: 516\n",
      "5: 271\n",
      "3: 47\n",
      "4: 131\n",
      "1: 105\n",
      "0: 515\n",
      "\n",
      "\n",
      "orginal data:\n",
      "2: 317\n",
      "5: 264\n",
      "0: 815\n",
      "3: 37\n",
      "4: 41\n",
      "1: 111\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "import collections\n",
    "from collections import Counter\n",
    "import re\n",
    "import csv\n",
    "import nltk\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "%matplotlib inline\n",
    "file_path = \"/Users/spb/desktop/2019-05-01-2021-05-29-Ethiopia copy.csv\"\n",
    "df = pd.read_csv(file_path, sep= ',', header=0)\n",
    "\n",
    "'''\n",
    "This section is straight from David's preprocessing \n",
    "'''\n",
    "def remove_num(list):\n",
    "    pattern = '[0-9]'\n",
    "    list = [re.sub(pattern, '', i) for i in list]\n",
    "    return list\n",
    "df[\"notes\"] = remove_num(df[\"notes\"])\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "df[\"notes\"] = df[\"notes\"].apply(lambda x: tokenizer.tokenize(x.lower()))\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "def word_lemmatizer(text):\n",
    "    lem_text = [lemmatizer.lemmatize(i) for i in text]\n",
    "    return lem_text\n",
    "df[\"notes\"] = df[\"notes\"].apply(lambda x: word_lemmatizer(x))\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    words = [w for w in text if w not in stopwords.words('english')]\n",
    "    return words\n",
    "df[\"notes\"] = df[\"notes\"].apply(lambda x: remove_stopwords(x))\n",
    "\n",
    "def remove_month(text):\n",
    "    dates = ['january', 'feburary', 'march','april','may','june','july','august',\n",
    "             'september','october','november','december']\n",
    "    words =[w for w in text if w not in dates]\n",
    "    return words\n",
    "df[\"notes\"] = df[\"notes\"].apply(lambda x: remove_month(x))\n",
    "\n",
    "\n",
    "'''\n",
    "This is the way I made separate corpuses for each event_type\n",
    "'''\n",
    "protests_df = df.loc[df['event_type'] == \"Protests\"]\n",
    "df1 = protests_df['notes']\n",
    "pro_tolist = df1.to_numpy().tolist()\n",
    "pro_txt = \" \".join(str(x) for x in pro_tolist)\n",
    "\n",
    "battles_df = df.loc[df['event_type'] == \"Battles\"]\n",
    "df1 = battles_df['notes']\n",
    "bat_tolist = df1.to_numpy().tolist()\n",
    "bat_txt = \" \".join(str(x) for x in bat_tolist)\n",
    "\n",
    "exremvio_df = df.loc[df['event_type'] == \"Explosions/Remote violence\"]\n",
    "df1 = exremvio_df['notes']\n",
    "exremvio_tolist = df1.to_numpy().tolist()\n",
    "exremvio_txt = \" \".join(str(x) for x in exremvio_tolist)\n",
    "\n",
    "riots_df = df.loc[df['event_type'] == \"Riots\"]\n",
    "df1 = riots_df['notes']\n",
    "riots_tolist = df1.to_numpy().tolist()\n",
    "riots_txt = \" \".join(str(x) for x in riots_tolist)\n",
    "\n",
    "stratdev_df = df.loc[df['event_type'] == \"Strategic developments\"]\n",
    "df1 = stratdev_df['notes']\n",
    "stratdev_tolist = df1.to_numpy().tolist()\n",
    "stratdev_txt = \" \".join(str(x) for x in stratdev_tolist)\n",
    "\n",
    "vaciv_df = df.loc[df['event_type'] == \"Violence against civilians\"]\n",
    "df1 = vaciv_df['notes']\n",
    "vaciv_tolist = df1.to_numpy().tolist()\n",
    "vaciv_txt = \" \".join(str(x) for x in vaciv_tolist)\n",
    "\n",
    "\n",
    "'''\n",
    "Placed all the text into a list format for my matrices\n",
    "'''\n",
    "corpus_total = [pro_txt, bat_txt, exremvio_txt, riots_txt, stratdev_txt, vaciv_txt]\n",
    "\n",
    "\n",
    "'''\n",
    "vectorized data\n",
    "'''\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(corpus_total)\n",
    "\n",
    "vectorizer1 = TfidfVectorizer()\n",
    "Y = vectorizer1.fit_transform(corpus_total)\n",
    "\n",
    "terms = vectorizer1.get_feature_names()\n",
    "total_vocab = vectorizer1.vocabulary_\n",
    "\n",
    "'''\n",
    "created different matrices\n",
    "tfidf matrix:\n",
    "'''\n",
    "dfmatrix1 = pd.DataFrame(Y.toarray().transpose(),\n",
    "                   index=terms)\n",
    "'''\n",
    "term_freq. matrix:\n",
    "'''\n",
    "dfmatrix2 = pd.DataFrame(X.toarray().transpose(),\n",
    "                   index=terms)\n",
    "'''\n",
    "set column names for these matrices based on the order in which I placed text in the list (corpus_total)\n",
    "'pro_doc' = Protests\n",
    "'bat_doc' = Battles\n",
    "'erv_doc' = Explosions/remote viol.\n",
    "'riot_doc' = Riots\n",
    "'strdev_doc' = strategic development\n",
    "'vac_doc' = violence against civs\n",
    "\n",
    "'''\n",
    "dfmatrix1.columns = ['pro_doc','bat_doc','erv_doc', 'riot_doc', 'strdev_doc', 'vac_doc']\n",
    "dfmatrix2.columns = ['pro_doc','bat_doc','erv_doc', 'riot_doc', 'strdev_doc', 'vac_doc']\n",
    "\n",
    "print('tf-idf_score_matrix')\n",
    "print(dfmatrix1)\n",
    "print('term_freq(tf)_matrix')\n",
    "print(dfmatrix2)\n",
    "\n",
    "'''\n",
    "I used these files to ultimately map my cluster #s to event_types\n",
    "'''\n",
    "# dfmatrix1.to_csv(\"/Users/spb/desktop/tfidf_takealook.csv\")\n",
    "# dfmatrix2.to_csv(\"/Users/spb/desktop/doc_term_takealook.csv\")\n",
    "\n",
    "\n",
    "'''\n",
    "I pre-set in # of clusters to 6 and fit the model to my corpus_total\n",
    "Created cluster centriods that would be used to find words that best fit the particular cluster\n",
    "'''\n",
    "true_k = 6\n",
    "model = KMeans(n_clusters=true_k, init='k-means++', max_iter=300, n_init=3, random_state=10).fit(Y)\n",
    "order_centroids = model.cluster_centers_.argsort()[:, ::-1]\n",
    "\n",
    "'''\n",
    "Created a visual showing top tf-idf words(showing 10 for example) for each cluster. They are listed highest \n",
    "to lowest score in each cluster segment \n",
    "'''\n",
    "def word_clustering(k_num, centriods, features):\n",
    "    for i in range(k_num):\n",
    "        print('Cluster %d:' % i),\n",
    "        for ind in centriods[i, :10]:\n",
    "            print(' %s' % features[ind])\n",
    "word_clustering(true_k, order_centroids, terms)\n",
    "\n",
    "'''\n",
    "Here I attempt to load in the dataset to get a label prediction from the kmeans model setup, result is an array\n",
    "of label predictions 0 to 5\n",
    "'''\n",
    "notes_input = df[\"notes\"].tolist()\n",
    "res = [' '.join(ele) for ele in notes_input]\n",
    "char_remove = [re.compile(r'\\W*\\b\\w{1,2}\\b').sub(\"\", i) for i in res]\n",
    "test = vectorizer1.transform(char_remove)\n",
    "prediction = model.predict(test)\n",
    "\n",
    "'''\n",
    "After that I take the same data set predications were ran on and I covert the event types to #s 0 to 5 \n",
    "By labeling this here I am able to see what the model chose for each line in the entire document \n",
    "I was only able to map the cluster #s to the event type by printing the clusters words (pervious) and \n",
    "searching for thier scores in a exported cvs file\n",
    "'''\n",
    "df_raw = pd.read_csv(file_path, sep= ',', header=0)\n",
    "df1_raw = df_raw.dropna(axis=0, how='any', thresh=None, subset=['notes'], inplace=False)\n",
    "df2_raw = df1_raw.event_type.replace({\"Violence against civilians\": 2, \"Battles\": 0, 'Protests': 5, \n",
    "                             'Strategic developments': 3, 'Explosions/Remote violence': 4, 'Riots': 1})\n",
    "orginal_array = df2_raw.to_numpy()\n",
    "pred_array_cleaned = prediction\n",
    "\n",
    "'''\n",
    "This is my rudimentary way of counting all the event types of the actual file and then counting all the \n",
    "predictions of that file\n",
    "\n",
    "I subtracted actaul from prediction count numbers. Sadly it has only proven to be 65% to 70% accurate depending \n",
    "on size of the intial document ingested. Using roughly two years worth of data had the highest accuracy \n",
    "compared to 20 years or even 6 months.\n",
    "'''\n",
    "elements_count = collections.Counter(pred_array_cleaned)\n",
    "elements_count1 = collections.Counter(orginal_array)\n",
    "print('\\n')\n",
    "print('prediction data:')\n",
    "for key, value in elements_count.items():\n",
    "    print(f\"{key}: {value }\")\n",
    "print('\\n')\n",
    "print('orginal data:')\n",
    "for key1, value1 in elements_count1.items():\n",
    "    print(f\"{key1}: {value1}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29690ae0-c932-4471-8646-fc61cce0bc7a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5459507-8df6-4a96-93b1-173fd65ab8c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a921a5-9802-40a9-a887-5de6c77c548d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
